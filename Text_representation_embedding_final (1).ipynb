{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Classic 3](#Classic_3)\n",
    "- [NG5](#NG5)\n",
    "- [NG20](#NG20)\n",
    "- [Reuters8](#Reuteurs_8)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np#utils\n",
    "import os     \n",
    "\n",
    "from gensim.models import Word2Vec#gensim\n",
    "from gensim.models import FastText\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer#sci-kit learn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic_3\n",
    "## Data\n",
    "On se place au même niveau que le dossier classic qui contient toutes les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(\"classic/\")   \n",
    "print(all_files)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_files)):\n",
    "    all_files[i]=\"classic/\"+all_files[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_labels=[]\n",
    "start=len(\"classic/\")\n",
    "for path in all_files:\n",
    "    text_labels.append(path[start:start+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels[0:8\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict={'cisi':0,'cran':1,'med.':2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le document n°47 (med.000423) est vide => on vérifie la taille du document avant d'ajouter le label, et, ainsi on ne tient plus compte de ce document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=[]\n",
    "for file_path in all_files:\n",
    "    with open(file_path) as file:\n",
    "        doc=[]\n",
    "        for line in file:\n",
    "            doc+=simple_preprocess(remove_stopwords(line))\n",
    "        data.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data=[]\n",
    "for doc in data:\n",
    "    joined_data.append(\" \".join(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_files),len(data),len(joined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for i,label in enumerate(text_labels):\n",
    "    if len(data[i])!=0:\n",
    "        labels.append(label_dict[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Document Frequency Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Commenter tous les paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer(input='content',lowercase=False, use_idf=True,min_df=1, sublinear_tf=False)\n",
    "matrix=tf.fit_transform(joined_data)\n",
    "vocabulary = tf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(matrix[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot='developing'\n",
    "index_mot=vocabulary[mot]\n",
    "index_doc=0\n",
    "np.array(matrix[index_doc].todense())[0][index_mot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "        data,\n",
    "        size=150,#taile du vecteur d'embedding pour chaque mot\n",
    "        window=10,\n",
    "        min_count=1,\n",
    "        workers=10)\n",
    "model.train(data, total_examples=len(data), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=\"disorders\",topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\n",
    "        data,\n",
    "        size=150,#taille du vecteur d'embedding pour chaque mot\n",
    "        window=10,\n",
    "        min_count=1,\n",
    "        workers=10)\n",
    "model.build_vocab(data,update=True)\n",
    "model.train(data, total_examples=len(data), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sans tenir compte de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\n",
    "for document in data:\n",
    "    D_doc=[]\n",
    "    for mot in document:\n",
    "        D_doc.append(model.wv.word_vec(mot))\n",
    "    D.append(D_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  En tenant compte de TF-IDF\n",
    "Sinon commenter indexmot, valeurtf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\n",
    "for index_doc, document in enumerate(data):\n",
    "    D_doc=[]\n",
    "    for mot in document:\n",
    "        index_mot=vocabulary[mot]\n",
    "        valeur_tf_idf=np.array(matrix[index_doc].todense())[0][index_mot]\n",
    "        D_doc.append(valeur_tf_idf*model.wv.word_vec(mot))\n",
    "    D.append(D_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(D),len(D[0]),len(D[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, document in enumerate(D):\n",
    "    if len(document)==0:\n",
    "        print(\"doc vide\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D[47],data[47]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le document n°47 (med.000423) est vide => on vérifie la taille du document avant de faire la transformation SVD afin de ne pas avoir d'erreur, et, ainsi on ne tient plus compte de ce document (i.e. on ne l'ajoute pas dans la matrice M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2=[]### document représenté par u1 \n",
    "M3=[]### document représenté par u1 + u2 \n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "\n",
    "for document in D:\n",
    "    if len(document)!=0:\n",
    "        svd.fit(document)\n",
    "\n",
    "        M2.append(svd.components_[0])#[0] to transform 2D array to 1D\n",
    "\n",
    "        M3.append(svd.components_[0]+svd.components_[1])#u1 + u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a 3890 doc au lieu de 3891 à cause du document vide "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3,\n",
    "               init='k-means++',\n",
    "               n_init=10,\n",
    "               n_jobs=-1).fit(M3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans = kmeans.predict(M3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.3f\" % adjusted_rand_score(labels, y_kmeans), \"%.3f\" % accuracy_score(labels, y_kmeans), \"%.3f\" % normalized_mutual_info_score(labels, y_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant le preproccess on avait  \n",
    "(0.731, 0.234, 0.659) pour u1\n",
    "\n",
    "et  \n",
    "(0.676, 0.0589, 0.591) pour u1+u2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NG5\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = [\n",
    "'rec.motorcycles',\n",
    "'rec.sport.baseball',\n",
    "'comp.graphics',\n",
    "'sci.space',\n",
    "'talk.politics.mideast'\n",
    "]\n",
    "ng5 = fetch_20newsgroups(categories=categories)\n",
    "documents = ng5.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=ng5.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for doc in documents:\n",
    "    data.append(simple_preprocess(remove_stopwords(doc)))\n",
    "print(len(data),\"\\n\",data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0],ng5.target_names[labels[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Document Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data=[]\n",
    "for doc in data:\n",
    "    joined_data.append(\" \".join(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer(input='content',lowercase=False, use_idf=True,min_df=1, sublinear_tf=False)\n",
    "matrix=tf.fit_transform(joined_data)\n",
    "vocabulary = tf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "        data,\n",
    "        size=150,#taille du vecteur d'embedding pour chaque mot\n",
    "        window=10,\n",
    "        min_count=1,\n",
    "        workers=10)\n",
    "model.train(data, total_examples=len(data), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=\"team\",topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\n",
    "        data,\n",
    "        size=150,#taille du vecteur d'embedding pour chaque mot\n",
    "        window=10,\n",
    "        min_count=1,\n",
    "        workers=10)\n",
    "model.build_vocab(data,update=True)\n",
    "model.train(data, total_examples=len(data), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sans tenir compte de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\n",
    "for document in data:\n",
    "    D_doc=[]\n",
    "    for mot in document:\n",
    "        D_doc.append(model.wv.word_vec(mot))\n",
    "    D.append(D_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  En tenant compte de TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\n",
    "for index_doc, document in enumerate(data):\n",
    "    D_doc=[]\n",
    "    for mot in document:\n",
    "        index_mot=vocabulary[mot]\n",
    "        valeur_tf_idf=np.array(matrix[index_doc].todense())[0][index_mot]\n",
    "        D_doc.append(valeur_tf_idf*model.wv.word_vec(mot))\n",
    "    D.append(D_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(D),len(D[0]),len(D[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, document in enumerate(D):\n",
    "    if len(document)==0:\n",
    "        print(\"doc vide\", idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas de document vide cette fois ci !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2=[]### document représenté par u1 \n",
    "M3=[]### document représenté par u1 + u2 \n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "\n",
    "for document in D:\n",
    "    svd.fit(document)\n",
    "    \n",
    "    M2.append(svd.components_[0])#[0] to transform 2D array to 1D\n",
    "    \n",
    "    M3.append(svd.components_[0]+svd.components_[1])#u1 + u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5,\n",
    "               init='k-means++',\n",
    "               n_init=10,\n",
    "               n_jobs=-1).fit(M3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans = kmeans.predict(M3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.3f\" % adjusted_rand_score(labels, y_kmeans), \"%.3f\" % accuracy_score(labels, y_kmeans), \"%.3f\" % normalized_mutual_info_score(labels, y_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant le preprocessing, on avait  \n",
    "(ARI, Clustering Accuracy et NMI)  \n",
    "(0.0454, 0.194, 0.0620)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NG20\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "ng20 = fetch_20newsgroups()\n",
    "documents = ng20.data\n",
    "labels=ng20.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for doc in documents:\n",
    "    data.append(simple_preprocess(remove_stopwords(doc)))\n",
    "print(len(data),\"\\n\",data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0],ng20.target_names[labels[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Document Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data=[]\n",
    "for doc in data:\n",
    "    joined_data.append(\" \".join(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer(input='content',lowercase=False, use_idf=True,min_df=1, sublinear_tf=False)\n",
    "matrix=tf.fit_transform(joined_data)\n",
    "vocabulary = tf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "        data,\n",
    "        size=150,#taille du vecteur d'embedding pour chaque mot\n",
    "        window=10,\n",
    "        min_count=1,\n",
    "        workers=10)\n",
    "model.train(data, total_examples=len(data), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=\"team\",topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\n",
    "        data,\n",
    "        size=150,#taille du vecteur d'embedding pour chaque mot\n",
    "        window=10,\n",
    "        min_count=1,\n",
    "        workers=10)\n",
    "model.build_vocab(data,update=True)\n",
    "model.train(data, total_examples=len(data), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sans tenir compte de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\n",
    "for document in data:\n",
    "    D_doc=[]\n",
    "    for mot in document:\n",
    "        D_doc.append(model.wv.word_vec(mot))\n",
    "    D.append(D_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  En tenant compte de TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\n",
    "for index_doc, document in enumerate(data):\n",
    "    D_doc=[]\n",
    "    for mot in document:\n",
    "        index_mot=vocabulary[mot]\n",
    "        valeur_tf_idf=np.array(matrix[index_doc].todense())[0][index_mot]\n",
    "        D_doc.append(valeur_tf_idf*model.wv.word_vec(mot))\n",
    "    D.append(D_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(D),len(D[0]),len(D[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, document in enumerate(D):\n",
    "    if len(document)==0:\n",
    "        print(\"doc vide\", idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas de document vide cette fois ci !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2=[]### document représenté par u1 \n",
    "M3=[]### document représenté par u1 + u2 \n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "\n",
    "for document in D:\n",
    "    svd.fit(document)\n",
    "    \n",
    "    M2.append(svd.components_[0])#[0] to transform 2D array to 1D\n",
    "    \n",
    "    M3.append(svd.components_[0]+svd.components_[1])#u1 + u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=20,\n",
    "               init='k-means++',\n",
    "               n_init=10,\n",
    "               n_jobs=-1).fit(M3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans = kmeans.predict(M3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.3f\" % adjusted_rand_score(labels, y_kmeans), \"%.3f\" % accuracy_score(labels, y_kmeans), \"%.3f\" % normalized_mutual_info_score(labels, y_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuteurs_8\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(\"./reuters8_data\")   \n",
    "\n",
    "print(all_files)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file  = open( \"./reuters8_data/reuters8_labels.txt\" );\n",
    "text_labels=[]\n",
    "\n",
    "for line in label_file.readlines():\n",
    "    \n",
    "    y =  line.splitlines()\n",
    "    \n",
    "    text_labels.append(y[0])\n",
    "\n",
    "len(text_labels)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels[0:10]\n",
    "unique_labels=set(text_labels)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict= dict(zip(unique_labels, range(0,8)))#{'cisi':0,'cran':1,'med.':2}\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for label in text_labels:\n",
    "    labels.append(label_dict[label])\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "\n",
    "f = open('./reuters8_data/reuters8_raw.txt', 'r')\n",
    "    \n",
    "tmp = f.read()\n",
    "tmp=tmp.split(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dernier document est vide !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[5485]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.pop(5485)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for doc in tmp:\n",
    "    data.append(simple_preprocess(remove_stopwords(doc)))\n",
    "print(len(data),\"\\n\",data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0],text_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Document Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_data=[]\n",
    "for doc in data:\n",
    "    joined_data.append(\" \".join(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer(input='content',lowercase=False, use_idf=True,min_df=1, sublinear_tf=False)\n",
    "matrix=tf.fit_transform(joined_data)\n",
    "vocabulary = tf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "        data,\n",
    "        size=150,#taille du vecteur d'embedding pour chaque mot\n",
    "        window=10,\n",
    "        min_count=1,\n",
    "        workers=10)\n",
    "model.train(data, total_examples=len(data), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=\"products\",topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(\n",
    "        data,\n",
    "        size=150,#taille du vecteur d'embedding pour chaque mot\n",
    "        window=10,\n",
    "        min_count=1,\n",
    "        workers=10)\n",
    "model.build_vocab(data,update=True)\n",
    "model.train(data, total_examples=len(data), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sans tenir compte de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\n",
    "for document in data:\n",
    "    D_doc=[]\n",
    "    for mot in document:\n",
    "        D_doc.append(model.wv.word_vec(mot))\n",
    "    D.append(D_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  En tenant compte de TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\n",
    "for index_doc, document in enumerate(data):\n",
    "    D_doc=[]\n",
    "    for mot in document:\n",
    "        index_mot=vocabulary[mot]\n",
    "        valeur_tf_idf=np.array(matrix[index_doc].todense())[0][index_mot]\n",
    "        D_doc.append(valeur_tf_idf*model.wv.word_vec(mot))\n",
    "    D.append(D_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(D),len(D[0]),len(D[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, document in enumerate(D):\n",
    "    if len(document)==0:\n",
    "        print(\"doc vide\", idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas de document vide cette fois ci !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2=[]### document représenté par u1 \n",
    "M3=[]### document représenté par u1 + u2 \n",
    "svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "\n",
    "for document in D:\n",
    "    svd.fit(document)\n",
    "    \n",
    "    M2.append(svd.components_[0])#[0] to transform 2D array to 1D\n",
    "    \n",
    "    M3.append(svd.components_[0]+svd.components_[1])#u1 + u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8,\n",
    "               init='k-means++',\n",
    "               n_init=10,\n",
    "               n_jobs=-1).fit(M3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans = kmeans.predict(M3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.3f\" % adjusted_rand_score(labels, y_kmeans), \"%.3f\" % accuracy_score(labels, y_kmeans), \"%.3f\" % normalized_mutual_info_score(labels, y_kmeans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### En utilisant Word2Vec\n",
    "\n",
    "Methode (ARI, Clustering Accuracy et NMI)/Dataset | Classic | NG5 | NG20 | R8  \n",
    "--- | --- | --- | --- | ---  \n",
    "document représenté par u 1 sans utiliser tf-idf | (0.731, 0.234, 0.659)| (0.120, 0.229, 0.252) | (0.103, 0.048, 0.235) |  0.397 0.010 0.538 \n",
    "document représenté par u 1 en utilisant tf-idf | (0.922, 0.264, 0.877) | (0.450, 0.193, 0.463) | (0.175, 0.033, 0.305) |   0.369 0.014 0.496\n",
    "document représenté par u 1 + u 2 sans utiliser tf-idf | (0.672, 0.268, 0.617) | (0.144, 0.148, 0.199) | (0.062, 0.060, 0.163) |  (0.200, 0.071, 0.305)\n",
    "document représenté par u 1 + u 2 en utilisant tf-idf | (0.705, 0.0576, 0.630) | (0.320, 0.219, 0.330) | (0.146, 0.0597, 0.264) |  0.188 0.051 0.331"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En utilisant fastText\n",
    "\n",
    "Methode (ARI, Clustering Accuracy et NMI)/Dataset | Classic | NG5 | NG20 | R8  \n",
    "--- | --- | --- | --- | ---  \n",
    "document représenté par u 1 sans utiliser tf-idf | 0.923 0.364 0.884 | 0.039 0.199 0.069 | 0.023 0.049 0.077 | 0.358 0.137 0.469\n",
    "document représenté par u 1 en utilisant tf-idf  | 0.871 0.955 0.813| 0.187 0.215 0.240 | 0.107 0.049 0.217 | 0.345 0.130 0.453\n",
    "document représenté par u 1 + u 2 sans utiliser tf-idf  | 0.766 0.037 0.710 | 0.029 0.198 0.034 | 0.015 0.041 0.055 | 0.210 0.112 0.324\n",
    "document représenté par u 1 + u 2 en utilisant tf-idf | 0.745 0.907 0.667 | 0.183 0.095 0.239 | 0.085 0.085 0.182 | 0.237 0.271 0.342"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Classic 3](#Classic_3)\n",
    "- [NG5](#NG5)\n",
    "- [NG20](#NG20)\n",
    "- [Reuters8](#Reuteurs_8)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
